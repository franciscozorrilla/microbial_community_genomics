{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Hands-on session on genome mining\n",
    "\n",
    "The goal of this tutorial is to provide a brief introduction to bioinformatic approaches for genome mining. \n",
    "\n",
    "The index of these notebook includes:\n",
    "0. [Python introduction](#0.-Python-introduction)\n",
    "1. [Gene annotation](#1.-Gene-annotation)\n",
    "2. [Alignment-based approaches](#2.-Alignment-based-approaches)\n",
    "3. [HMMs-based approaches](#3.-HMMs-based-approaches)\n",
    "4. [Feature-based approaches](#4.-Feature-based-approaches)\n",
    "5. [Genome-scale Metabolic Models](#5.-GEMs)\n",
    "\n",
    "**Notes on notebooks usage**\n",
    "\n",
    "You are working in a `jupyter notebook`. These documents allow to both write executable code, for example in `python`, and document it using `markdown` in different `cells`. \n",
    "\n",
    "Some useful shortcuts include:\n",
    "- Shift+Enter : execute cell\n",
    "- Esc, b : add a cell below\n",
    "- Esc, a : add a cell above\n",
    "- Esc, m : transform cell to `markdown`\n",
    "- Esc, d : delete cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Gene annotation \n",
    "\n",
    "This section will introduce you to sequence handling in Python, covering DNA, RNA, proteins, and coding a basic Open Reading Frame (ORF) finder. By the end, you'll be familiar with using Biopython for sequence manipulation and creating a simple ORF detection function.\n",
    "\n",
    "Core Concepts\n",
    "- DNA (Deoxyribonucleic Acid): The molecular blueprint of organisms. Consists of nucleotides represented by the letters A, T, C, and G.\n",
    "- RNA (Ribonucleic Acid): Transcribed from DNA and serves as a template for protein synthesis. It contains U (Uracil) instead of T.\n",
    "- Protein: Composed of amino acids, translated from RNA through codons (triplets of nucleotides).\n",
    "- Open Reading Frames (ORFs): DNA segments that have the potential to code for proteins. ORFs start with a start codon (usually ATG) and end with a stop codon (e.g., TAA, TAG, TGA).\n",
    "\n",
    "![title](img/orf.png)\n",
    "\n",
    "\n",
    "## 1.1. Working with DNA Sequences in Python\n",
    "\n",
    "First, let's load and manipulate DNA sequences using basic Python functions and Biopython.\n",
    "Example DNA Sequence\n",
    "\n",
    "Let's start with a small sample DNA sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dna = \"ATGCGATACGCTTGA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Converting DNA to RNA\n",
    "\n",
    "To convert DNA to RNA, replace thymine (T) with uracil (U)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Basic ORF Finder\n",
    "\n",
    "An ORF finder identifies sequences between a start codon (ATG) and a stop codon (TAA, TAG, TGA). Here’s a simple function to find ORFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orfs(dna_sequence):\n",
    "    \"\"\" \n",
    "    This function scans the sequence for ATG, then continues \n",
    "    in triplets until it finds a stop codon. \n",
    "    It then extracts and saves each ORF. \n",
    "    \"\"\"\n",
    "    start_codon = \"ATG\"\n",
    "    stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
    "    orfs = []\n",
    "\n",
    "    # Complete the code :)\n",
    "    \n",
    "    return orfs\n",
    "\n",
    "# Test the ORF finder with the sample DNA\n",
    "orfs = find_orfs(sample_dna)\n",
    "print(\"ORFs Found:\")\n",
    "for orf in orfs:\n",
    "    print(orf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Using Biopython for Sequence Handling\n",
    "\n",
    "Biopython’s Seq object offers powerful tools for sequence manipulation. Here’s how to create sequences and translate them to protein sequences.\n",
    "Example: Creating and Translating a Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "\n",
    "# Create a Seq object\n",
    "dna_seq = Seq(\"ATGCGTCTAA\")\n",
    "\n",
    "# Translate the DNA sequence to a protein\n",
    "protein_seq = dna_seq.translate()\n",
    "print(f\"Protein Sequence: {protein_seq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Loading a genome with Biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Load the sequence from a FASTA file\n",
    "with open(\"./data/test_genome.fna\") as file:\n",
    "    sequence_record = SeqIO.read(file, \"fasta\")\n",
    "\n",
    "# Display the sequence and its metadata\n",
    "print(f\"ID: {sequence_record.id}\")\n",
    "print(f\"Description: {sequence_record.description}\")\n",
    "print(f\"Sequence: {sequence_record.seq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Putting It All Together\n",
    "\n",
    "Here’s a final example combining all the steps into a small ORF finder that translates found ORFs into proteins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "\n",
    "def find_and_translate_orfs(dna_sequence):\n",
    "    start_codon = \"ATG\"\n",
    "    stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
    "    orf_proteins = []\n",
    "\n",
    "    # Complete the code\n",
    "\n",
    "    return orf_proteins\n",
    "\n",
    "# Test with a longer DNA sequence\n",
    "proteins = find_and_translate_orfs(sequence_record.seq)\n",
    "print(\"Translated Proteins from ORFs:\")\n",
    "for protein in proteins:\n",
    "    print(protein)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Using Biopython for multifasta Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Biopython if needed: !pip install biopython\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Sample DNA sequence\n",
    "dna_sequence = Seq(\"ATGCTAGCTAGCTCGTAGCT\")\n",
    "print(\"Sequence:\", dna_sequence)\n",
    "print(\"Reverse Complement:\", dna_sequence.reverse_complement())\n",
    "print(\"Protein Translation:\", dna_sequence.translate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a FASTA file (replace with actual file path if needed)\n",
    "for record in SeqIO.parse(\"./data/test_contigs.fna\", \"fasta\"):\n",
    "    print(f\"ID: {record.id}, Length: {len(record.seq)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Load sequences from a FASTA file into a dictionary, compute the reverse complement, and translate to protein and create a new dictionary with the structure {id:[dna_seq, revcomp_seq, aa_seqs]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "--------\n",
    "\n",
    "# 2. Alignment-based approaches\n",
    "\n",
    "## 2.1. Introduction to Sequence Homology and Identity\n",
    "\n",
    "Objective: Explain the concepts of homology and sequence identity, crucial in alignment-based approaches.\n",
    "\n",
    "- Sequence Homology: Similarity due to shared ancestry; can be orthologous (same function in different species) or paralogous (functionally diverged post-duplication).\n",
    "- Sequence Identity: Percentage of identical residues in the alignment of two sequences, helping to quantify similarity.\n",
    "\n",
    "Assumption: we can infer function of a protein by homology given with a set of sequences with known functions.\n",
    "\n",
    "## 2.2. Position weight matrices\n",
    "\n",
    "In this example we will create a Position Weight Matrix (PWM) for the sequence \"AGN[A|G]GG\", use it to scan a larger DNA sequence, and plot the probability of finding \"AGGAGG\" at each position.\n",
    "\n",
    "**Step 1**: Define the PWM for \"AGGAGG\"\n",
    "A Position Weight Matrix quantifies the likelihood of each nucleotide at each position in the motif. For this example, we'll assume a simple PWM based on \"AGGAGG\" alone, assigning high probabilities to the observed bases in this motif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the target sequence\n",
    "target_motif = \"AGGAGG\"\n",
    "\n",
    "# Define PWM for a motif of 6 bases (length of AGGAGG)\n",
    "# Rows: A, C, G, T. Columns correspond to each position in \"AGGAGG\"\n",
    "pwm = np.array([\n",
    "    [1, 0.25, 0.5, 0, 1, 0],  # Probability of 'A' at each position\n",
    "    [0, 0.25, 0  , 0, 0, 0],  # Probability of 'C' at each position\n",
    "    [0, 0.25, 0.5, 0, 0, 1],  # Probability of 'G' at each position\n",
    "    [0, 0.25, 0  , 1, 0, 0]   # Probability of 'T' at each position\n",
    "], dtype=float)\n",
    "\n",
    "# To handle probabilities in a real PWM, add a small pseudocount to avoid zeros\n",
    "pwm = (pwm + 0.01) / pwm.sum(axis=0)  # Normalize each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Calculate the Match Score for Each Window\n",
    "Define a function to calculate the probability of observing the pattern in different windows across a larger DNA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pwm_score(sequence, pwm):\n",
    "    scores = []\n",
    "    for i in range(len(sequence) - pwm.shape[1] + 1):\n",
    "        window = sequence[i:i + pwm.shape[1]]\n",
    "        score = 1.0\n",
    "        for j, nucleotide in enumerate(window):\n",
    "            if nucleotide == 'A':\n",
    "                score *= pwm[0, j]\n",
    "            elif nucleotide == 'C':\n",
    "                score *= pwm[1, j]\n",
    "            elif nucleotide == 'G':\n",
    "                score *= pwm[2, j]\n",
    "            elif nucleotide == 'T':\n",
    "                score *= pwm[3, j]\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Apply the PWM to a Test Sequence\n",
    "Define a test DNA sequence, apply the PWM, and compute the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test sequence\n",
    "test_sequence = \"TTTAGGAGGCTAGGAGGATGGAGGTTAGGAGGGT\"\n",
    "\n",
    "# Calculate the scores for each window\n",
    "scores = calculate_pwm_score(test_sequence, pwm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Plot the Probability per Window\n",
    "Finally, plot the results to visualize the probability of \"AGGAGG\" appearing at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Running BLAST \n",
    "\n",
    "Core Concepts: BLAST (Basic Local Alignment Search Tool) compares sequences to find regions of similarity. We’ll use the Biopython NCBIWWW module to run BLAST against NCBI’s online database."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "\n",
    "# Input sequence to BLAST\n",
    "sequence = Seq(\"ATGCTAGCTAGCTCGTAGCT\")\n",
    "\n",
    "# Perform BLAST query (this may take a few moments)\n",
    "result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", sequence)\n",
    "print(\"BLAST query complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running BLAST on NCBI servers may have rate limits. Optionally, download and run BLAST locally for larger datasets.\n",
    "\n",
    "### 2.3.1 Running BLAST locally\n",
    "\n",
    "Open a new terminal and run the following commands\n",
    "\n",
    "```{bash}\n",
    "\n",
    "# Create a dictionary\n",
    "cd s07_hands_on\n",
    "mkdir blast_exercise\n",
    "\n",
    "# Load blast \n",
    "ml BLAST+\n",
    "\n",
    "# Inspect the file to make the database\n",
    "head -20 ./data/PlasticDB.fasta \n",
    "\n",
    "# Make a database\n",
    "makeblastdb -in ./data/PlasticDB.fasta -dbtype prot -out ./blast_exercise/plasticdb -title plasticdb -parse_seqids\n",
    "\n",
    "# Run BlastP\n",
    "blastp -query ./data/omd2_candidate.faa -db ./data/blast_exercise/plasticdb -outfmt 6 -evalue 0.0001 -out ./data/omd2_candidate_blast.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Parsing BLAST Results and Checking Hits with Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the file with pandas and store it in blast_results variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems hard to interpret without a header... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results = pd.read_csv('./data/omd2_candidate_blast.out', sep='\\t', header=None)\n",
    "blast_results.columns = [\"qseqid\", \"sseqid\", \"pident\", \"length\", \"mismatch\", \n",
    "                        \"gapopen\", \"qstart\", \"qend\", \"sstart\", \"send\", \n",
    "                        \"evalue\", \"bitscore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Subset the previous results to get only results with an e-value <0.0000001.\n",
    "2. Further filter cases with pident >=50 and length >150.\n",
    "3. How many genes satisfy the condition 1 and 2? \n",
    "4. What are the different degrading enzymes identified? \n",
    "5. Can you do a plot showing something interesting about the different biodegradation categories? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "1. Load the genome collection (and metadata) from OMD2\n",
    "2. Can you identify the genus and species taxonomy of the organism we are analyzing?\n",
    "3. Would you trust the quality of the genome analyzed? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../s0506_python/data/genome_summary.csv.gz', compression='gzip', index_col=0)\n",
    "md = pd.read_csv('../s0506_python/data/metadata.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "----------------\n",
    "\n",
    "# 3. HMMs-based approaches\n",
    "\n",
    "Core concepts: Run multiple HMMs on a collection of sequences, explore concepts such as cluster completeness, and further developments such as antismash.\n",
    "\n",
    "We will use the operon Nif as example, important because of the process of nitrogen fixation:\n",
    "\n",
    "- Nitrogen is a limiting nutrient for plants, and although it makes up ~70% of the atmosphere, the gaseous form (N2) is unusable for them.\n",
    "- So how does nitrogen get from the atmosphere into a usable form? → “nitrogen fixation”\n",
    "  - bacteria can convert N2 into organic nutrients like ammonium (NH4+) and nitrate (NO3–) that are usable by plants. \n",
    "  - in the ocean, blue-green cyanobacteria are the most abundant type of bacteria to fix nitrogen. \n",
    "- Collectively, these organisms are called diazotrophs, and account for ~90% of natural nitrogen fixation.\n",
    "\n",
    "This process involves 3 main genes:\n",
    "\n",
    "![title](img/nifHDK.png)\n",
    "\n",
    "We have produced HMMs for a series of H, D, K proteins. Given the long evolutionary history of these proteins, Blast cannot capture all the diversity and HMMs, with their higher sensitivity, can help in predicting diazotrophs in our genome collection.\n",
    "\n",
    "To work with HMMs, we will us the python module for HMMER pyhmmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyhmmer\n",
    "import glob\n",
    "import pyhmmer.easel as easel\n",
    "import collections\n",
    "\n",
    "def retrieve_hits(seqs_path, hmms, fields=[\"query\", \"subject\", \"bitscore\", \"evalue\"]):\n",
    "\n",
    "    # Load cluster proteins\n",
    "    with pyhmmer.easel.SequenceFile(seqs_path, digital=True, alphabet=easel.Alphabet.amino()) as seqs_file:\n",
    "        proteins = seqs_file.read_block()\n",
    "\n",
    "    # Run HMMs\n",
    "    Result = collections.namedtuple(\"Result\", fields)\n",
    "\n",
    "    results = []\n",
    "    for hits in pyhmmer.hmmsearch(hmms, proteins, E=1):\n",
    "        cog = hits.query_name.decode()\n",
    "        for hit in hits:\n",
    "            if hit.included:\n",
    "                results.append(Result(hit.name.decode(), cog, hit.score, hit.evalue))\n",
    "\n",
    "    # Results --> df\n",
    "    hits_df = {}\n",
    "    c = 0\n",
    "    for i in results:\n",
    "        hits_df[c] = list(i)\n",
    "        c += 1\n",
    "    hits_df = pd.DataFrame.from_dict(hits_df, orient='index', columns=fields)\n",
    "    \n",
    "    return hits_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the previous code doing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load a collection of HMMs\n",
    "HMMS = []\n",
    "for fil in glob.glob('./data/hmms_nifHDK/*.hmm'):\n",
    "    with pyhmmer.plan7.HMMFile(fil) as hmm_file:\n",
    "        HMMS.append(hmm_file.read())\n",
    "HMMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieve_hits('./data/omd2_candidate.faa', HMMS)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think this bacteria is a diazotroph?\n",
    "\n",
    "Let's try a different set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_hits('./data/cyanobact.faa', HMMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think this bacteria is a diazotroph?\n",
    "- Does it has the 3 genes? \n",
    "- What would be your interpretation from the raw predictions? \n",
    "- What do you think has happened here evolutionary speaking? \n",
    "- What would be your interpretation from the predictions with evalue<5e-10? \n",
    "- What else you could try to validate your hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Working with antismash outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Given the antismash output from a genome given in the previous file:\n",
    "- How many BGCs the genome has?\n",
    "- What are the different products of those BGCs?\n",
    "- How many of each? \n",
    "- How many present a regulatory, resistance\tor transport gene? \n",
    "- You can produce some graphs to inspect the problem if you are done quick ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antismash = pd.read_csv('./data/omd2_candidate-antismash.tsv', sep='\\t')\n",
    "antismash.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Working with merged antismash outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omd_bgcs = pd.read_csv('./data/bgcs.csv.gz', compression='gzip')\n",
    "omd_bgcs['biosample'] = [i.split('_')[1] for i in omd_bgcs['GENOME']]\n",
    "md = pd.read_csv('./data/metadata.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omd_bgcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "- Filter high quality mags (completion >= 90, contamination <5) from the BGC table. \n",
    "- Which is the GTDB taxonomy with largest number of Biosynthetic Regions?.\n",
    "- Add metadata information as done in the previous python sessions.\n",
    "- Can you group genomes by biosample making the average of the columns: \n",
    "  - `['# Biosynthetic Regions', '# Biosynthetic Products', 'RiPPs', 'RiPPs:Proteusins', 'NRPS', 'PKSI', 'Other PKS', 'Terpenes', 'Saccharides', 'Other', 'temp']`\n",
    "- Can you plot a figure to explore if higher temperature relates to more biosynthetic regions? and by product?\n",
    "- Extras:\n",
    "  - can you make a barplot showing in the X axis the nr of products for the top 5 producers with their taxonomies as labels in the y-axis?\n",
    "  - can you make a map plot coloring by number of products? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Working with EggNOG outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg = pd.read_csv('/nfs/home/smiravet/bc/bc2024/s07_hands_on/data/eggnog_output.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "1. Explore together the eggnog output\n",
    "2. Make a function to parse the PFAM column subsetting Eukaryotic-like proteins\n",
    "    - ankyrin repeat\n",
    "    - tetratrico peptide repeat\n",
    "    - tetratricopeptide repeat\n",
    "    - WD40 region\n",
    "    - WD40-like\n",
    "    - Leucine-rich repeat\n",
    "3. What percentage of the total gene collection are ELPs? \n",
    "\n",
    "_Note_ use <your_str>.lower() or <your_str>.upper() to ensure the match is case insensitive! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "# 4. Feature based approaches \n",
    "\n",
    "## 4.1. Defining a Random Forest Classifier to identify AMPs\n",
    "\n",
    "Sometimes the features in a protein that allow for a specific function are not given based the sequence but other composed factors, such as hydrophobicity, secondary structure motifs, a single reactive amino acid, etc. \n",
    "\n",
    "For these cases, machine learning and feature-based approaches can be integrated to integrate and analyze based on a composition of numerical descriptors. Antimicrobial peptides are one of the classic examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import peptides\n",
    "\n",
    "# Define a function to load sequences from a FASTA file\n",
    "def load_sequences(fasta_file):\n",
    "    sequences = {}\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences[record.id] = str(record.seq)\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences from the fasta file\n",
    "training_seqs = load_sequences(\"./data/amps/training_set.faa\")  # Positive AMP sequences\n",
    "training_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore and interpret together the output. What does AMP and NAMP mean? \n",
    "\n",
    "Now check the code below and understand what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aas = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "cols = ['aliphatic_index', 'boman', 'charge'] + [f'count_{i}' for i in aas] + [f'freq_{i}' for i in aas] \n",
    "cols += ['hydrophobic_moment', 'hydrophobicity', 'instability_index', 'isoelectric_point', 'mass_shift']\n",
    "cols += ['molecular_weight', 'mz']\n",
    "def predict_additional(peptide):\n",
    "    aas = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    l = [peptide.aliphatic_index(), peptide.boman(), peptide.charge()]+[peptide.counts().get(i) for i in aas]+[peptide.frequencies().get(i) for i in aas]\n",
    "    l += [peptide.hydrophobic_moment(), peptide.hydrophobicity(), peptide.instability_index(), peptide.isoelectric_point(), peptide.mass_shift()]\n",
    "    l += [peptide.molecular_weight(), peptide.mz() ]\n",
    "    additional_feats = {k:v for k, v in zip(cols, l)}\n",
    "    return additional_feats\n",
    "\n",
    "def featurize_seq(seq):\n",
    "    peptide = peptides.Peptide(seq)\n",
    "    feats = peptide.descriptors()\n",
    "    feats.update(predict_additional(peptide))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurize_seq('MGMRMMFTVFLLVVLATTVVSIPSDRASDGRNAVVHERAPELVVTATTNCCGYNPMTICPPCMCTYSCPPKRKPGRRND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work together in producing code for the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seqs = load_sequences(\"./data/amps/training_set.faa\")  # Positive AMP sequences\n",
    "\n",
    "# Predict features for all the training sequences\n",
    "df = pd.DataFrame.from_dict({k:featurize_seq(v) for k, v in training_seqs.items()}, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label +/-\n",
    "df['label'] = [0 if 'NAMP' in i else 1 for i in df.index]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Train a Random Forest Model\n",
    "\n",
    "With the features prepared, we can split the data and train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we’ll plot the ROC curve using scikit-learn and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict probabilities for the validation set\n",
    "y_probs = rf_model.predict_proba(X_val)[:, 1]  # Probability of the positive class\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_val, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Test the Model on New Data\n",
    "\n",
    "Now, we can use the model to predict labels for the test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the test sequences\n",
    "to_test = load_sequences(\"./data/amps/test_set.faa\")  # Positive AMP sequences\n",
    "\n",
    "# Predict features for all the training sequences\n",
    "df2 = pd.DataFrame.from_dict({k:featurize_seq(v) for k, v in to_test.items()}, orient='index')\n",
    "\n",
    "# Predict the probability of each test sequence being positive\n",
    "test_probs = rf_model.predict_proba(df2)[:, 1]\n",
    "test_predictions = rf_model.predict(df2)\n",
    "\n",
    "# Display predictions\n",
    "for i, seq in enumerate(to_test):\n",
    "    print(f\"Sequence: {seq}\")\n",
    "    print(f\"Prediction: {'Positive' if test_predictions[i] == 1 else 'Negative'}\")\n",
    "    print(f\"Probability of being AMP: {test_probs[i]:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: think about potential visualizations of this data. What information could be relevant to highlight? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Exploring importances from a RF model\n",
    "\n",
    "Exercise 1: Identify and Visualize the Top 10 Most Important Features\n",
    "\n",
    "In this exercise, you’ll identify the top 10 most important features in the Random Forest model and visualize them to understand which features are most predictive of the positive class.\n",
    "\n",
    "    Calculate Feature Importance: Use the feature_importances_ attribute of the trained Random Forest model to get the importance scores of each feature.\n",
    "    Visualize the Top 10 Features: Plot a bar chart to show the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get feature importance from the model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "top_features = sorted_idx[:10]\n",
    "top_feature_names = [feature_names[i] for i in top_features]\n",
    "top_feature_importances = feature_importances[top_features]\n",
    "\n",
    "# Plot the top 10 important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_feature_names[::-1], top_feature_importances[::-1], color='skyblue')\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Compare Top Features between Positive and Negative Sets\n",
    "\n",
    "For the top 10 features identified, compare their distributions between the positive and negative sets. This helps you understand how these features differ across classes.\n",
    "\n",
    "Subset the Top Features: Select only the top 10 features from the data DataFrame.\n",
    "Plot Feature Distributions: For each of the top features, create a boxplot or violin plot to compare the distributions between positive and negative sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Subset data to include only top 10 features and labels\n",
    "top_features_data = df2[top_feature_names]\n",
    "top_features_data['label'] = test_predictions\n",
    "\n",
    "# Plot each feature as a boxplot to compare distributions in positive vs. negative sets\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, feature in enumerate(top_feature_names, 1):\n",
    "    plt.subplot(5, 2, i)\n",
    "    sns.boxplot(x='label', y=feature, data=top_features_data, palette=[\"#FFA07A\", \"#8FBC8F\"])\n",
    "    plt.title(f\"Distribution of {feature} by Class\")\n",
    "    plt.xlabel(\"Class (0 = Negative, 1 = Positive)\")\n",
    "    plt.ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- Feature Importance Plot: The bar chart shows the importance of each feature in distinguishing between classes, based on the Random Forest model.\n",
    "- Boxplots: Each boxplot displays the distribution of values for a given feature, with classes 0 (negative) and 1 (positive) shown separately.\n",
    "\n",
    "Additional Questions for Analysis\n",
    "\n",
    "- Which features show a clear separation between positive and negative sets?\n",
    "- Do any of the top features overlap heavily, indicating they may not be strong discriminators?\n",
    "- Can you hypothesize why certain features are predictive of positive or negative classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional: statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = top_features_data[['label', top_feature_names[0]]]\n",
    "subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "U1, p = mannwhitneyu(subdf[subdf['label']==1][top_feature_names[0]], subdf[subdf['label']==0][top_feature_names[0]], method=\"exact\")\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "- Iterate the top features listed in top_features_names and print <feat name> <p-value>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. OMD AMPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omd_amps = pd.read_csv('./data/amps/smorfs_summary.tsv.gz', compression='gzip', sep='\\t', index_col=0)\n",
    "omd_bgcs = pd.read_csv('./data/amps/morfs_bgcs_amps.tsv.gz', compression='gzip', sep='\\t', index_col=0)\n",
    "md = pd.read_csv('../s0506_python/data/metadata.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "- Explore together the given tables. \n",
    "- Add metadata information as done in the previous python sessions to the AMP table.\n",
    "- Filter high quality mags (completion >= 90, contamination <5) from the AMP table. \n",
    "- Which are the top 10 GTDB taxonomies with largest number of AMPs normalized by genome size?.\n",
    "- Can you group genomes by biosample making the average number of total AMPs and by each class? \n",
    "- Can you plot a figure to explore if GC content relates to more AMPs?\n",
    "- Extra: \n",
    "    - can you make a map plot coloring by number of products? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
